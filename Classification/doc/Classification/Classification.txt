namespace CGAL {
/*!

\mainpage User Manual
\anchor Chapter_Classification
\cgalAutoToc
\author Simon Giraudot, Florent Lafarge

This component implements the algorithm described in \cgalCite{cgal:lm-clscm-12} (section 2), generalized to handle multiple features and multiple labels. It classifies a data set into a user-defined set of labels, such as ground, vegetation and buildings. A flexible API is provided so that the user can classify any type of data, compute its own local features on the input data set and define its own labels based on these features.

\section Classification_Organization Package Organization

%Classification of data sets is achieved as follows (see Figure \cgalFigureRef{Classification_organization_fig}):

- some analysis is performed on the input data set
- features are computed based on this analysis
- a set of labels (for example: ground, building, vegetation) is defined by the user
- features are given weights and each pair of feature and label is assigned a relationship. This can either be done by hand or by automatic training if a set of inliers is given for each label (see \ref Classification_trainer)
- classification is computed itemwise by minimizing an energy defined as the sum of the values taken by features on input items (which depend on the feature-label relationship)
- additional regularization can be used by smoothing either locally or globally through an Alpha Expansion approach.

\cgalFigureBegin{Classification_organization_fig,organization.png}
Organization of the package.
\cgalFigureEnd

This package is designed to be easily extended by users: more specifically, features and labels can be defined by users to handle any data they need to classify (although \cgal provides a predefined framework for common urban scenes).

Currently, \cgal provides data structures to handle classification of point sets.

\section Classification_structures Data Structures

\subsection Classification_analysis Analysis

%Classification is based on the computation of local features. These features can take advantage of shared data structures that are precomputed and stored separately.

\cgal provides the following structures:

- [Point_set_neighborhood](@ref CGAL::Classification::Point_set_neighborhood) stores spatial searching structures and provides adapted queries for points
- [Local_eigen_analysis](@ref CGAL::Classification::Local_eigen_analysis) precomputes covariance matrices on local neighborhoods of points and stores the associated eigenvectors and eigenvalues
- [Planimetric_grid](@ref CGAL::Classification::Planimetric_grid) is a 2D grid used for digital terrain modeling.

The following code snippet shows how to instantiate such data structures from an input PLY point set (the full example is given at the end of the manual). The class `CGAL::Classifier` that handles classification is also instantiated (see \ref Classification_classifier).

\snippet Classification/example_classifier.cpp Analysis

\subsection Classification_features Features

Features are defined as scalar fields that associate each input item with a specific value. Users may want to define their own features, especially if the input data set comes with additional properties that were not anticipated by \cgal. A user-defined feature must inherit from `CGAL::Classification::Feature_base` and provide a method [value()](@ref CGAL::Classification::Feature_base::value) that associate a scalar value to each input item. Each feature has a weight that measure its strength with respect to the other features (see \ref Classification_labels).

Features are accessed through `Handle` objects, `CGAL::Classification::Feature_handle`.

\cgal provides some predefined features that are relevant for classification of urban point sets:

- [Distance_to_plane](@ref CGAL::Classification::Feature::Distance_to_plane) measures how far away a point is from a locally estimated plane
- [Elevation](@ref CGAL::Classification::Feature::Elevation) computes the local distance to an estimation of the ground
- [Vertical_dispersion](@ref CGAL::Classification::Feature::Vertical_dispersion) computes how noisy the point set is on a local Z-cylinder
- [Verticality](@ref CGAL::Classification::Feature::Verticality) compares the local normal vector to the vertical vector.

For more details about how these different features can help to identify one label or the other, please refer to their associated reference manual pages. In addition, \cgal also provides features solely based on the eigenvalues \cgalCite{cgal:mbrsh-raofw-11} of the local covariance matrix:

- [Anisotropy](@ref CGAL::Classification::Feature::Anisotropy)
- [Eigentropy](@ref CGAL::Classification::Feature::Eigentropy)
- [Linearity](@ref CGAL::Classification::Feature::Linearity)
- [Omnivariance](@ref CGAL::Classification::Feature::Omnivariance)
- [Planarity](@ref CGAL::Classification::Feature::Planarity)
- [Sphericity](@ref CGAL::Classification::Feature::Sphericity)
- [Sum_eigenvalues](@ref CGAL::Classification::Feature::Sum_eigenvalues)
- [Surface_variation](@ref CGAL::Classification::Feature::Surface_variation)

Finally, if the input data set has additional properties, these can also be used as features. For example, \cgal provides the following features:

- [Echo_scatter](@ref CGAL::Classification::Feature::Echo_scatter) uses the number of returns (echo) provided by most LIDAR scanners if available
- [Hsv](@ref CGAL::Classification::Feature::Hsv) uses input color information if available.

In the following code snippet, a subset of these features are instantiated and their respective weights are set. Note that these weights can also be automatically set up by training (see \ref Classification_trainer).

\snippet Classification/example_classifier.cpp Features

Users can define their own feature classes. Such classes must fulfill the following requirements:

- they must inherit `CGAL::Classification::Feature_base`
- their constructor(s) must take as first argument the item range used by the classifier object
- they must provide a method [value()](@ref CGAL::Classification::Feature_base::value) that associates each input point to a scalar value

The following example shows how to define a feature that discriminates
points that lie inside a 2D box from the others:

\cgalExample{Classification/example_feature.cpp}


\subsection Classification_labels Labels

A label represents how an item should be classified, for example: vegetation, building, road, etc. It is defined by the values the features are expected to take for a specific label. For example, vegetation is expected to have a high distance to plane and have a color close to green (if colors are available); facades have a low distance to plane and a low verticality; etc.

\cgal provides a class `CGAL::Classification::Label` to define such a set of feature effects, along with the associated `Handle` object: `CGAL::Classification::Label_handle`. Each label may define how a specific feature affects it:

- [FAVORING](@ref CGAL::Classification::Feature::FAVORING): the label is favored by high values of the feature
- [NEUTRAL](@ref CGAL::Classification::Feature::NEUTRAL): the label is not affected by the feature
- [PENALIZING](@ref CGAL::Classification::Feature::PENALIZING): the label is favored by low values of the feature

Let \f$x=(x_i)_{i=1..N_c}\f$ be a potential classification result with \f$N_c\f$ the number of input items and \f$x_i\f$ the class of the \f$i^{th}\f$ item (for example: vegetation, ground, etc.). Let \f$a_j(i)\f$ be the raw value of the \f$j^{th}\f$ feature at the \f$i^{th}\f$ item and \f$w_j\f$ be the weight of this feature. We define the normalized value \f$A_j(x_i) \in [0:1]\f$ of the \f$j^{th}\f$ feature at the \f$i^{th}\f$ item as follows:

\f{eqnarray*}{
    A_j(x_i) = & (1 - \min(\max(0,\frac{a_j(i)}{w_j}), 1)) & \mbox{if } a_j \mbox{ favors } x_i \\
    & 0.5 & \mbox{if } a_j \mbox{ is neutral for } x_i \\
    & \min(\max(0,\frac{a_j(i)}{w_j}), 1) & \mbox{if } a_j \mbox{ penalizes } x_i
\f}


The following code snippet shows how to add labels to the classification object and how to set the effects of the features on these labels. Note that these effects can also be automatically set up by training (see \ref Classification_trainer).

\snippet Classification/example_classifier.cpp Labels

\section Classification_classifier Classifier

%Classification is performed by minizing an energy over the input data set that may include regularization. \cgal provides 3 different methods for classification, ranging from high speed / low quality to low speed / high quality:

- `CGAL::Classifier::run()` 
- `CGAL::Classifier::run_with_local_smoothing()`
- `CGAL::Classifier::run_with_graphcut()`

On a point set of 3 millions points, the first method takes about 4 seconds, the second about 40 seconds and the third about 2 minutes.

\cgalFigureBegin{Classification_image,classif.png}
Top-Left: input point set. Top-Right: raw output classification represented by a set of colors (ground is orange, facades are blue, roofs are pink and vegetation is green). Bottom-Left: output classification using local smoothing. Bottom-Right: output classification using graphcut.
\cgalFigureEnd


Mathematical details are provided hereafter.

\subsection Classification_regularized_no No Regularization

- `CGAL::Classifier::run()`: this is the fastest method
that provides acceptable but usually noisy results (see Figure
\cgalFigureRef{Classification_image}, top-right).

Let \f$x=(x_i)_{i=1..N_c}\f$ be a potential classification result with \f$N_c\f$ the number of input items and \f$x_i\f$ the class of the \f$i^{th}\f$ item (for example: vegetation, ground, etc.). The classification is performed by minimizing the following energy:

\f[
  E(x) = \sum_{i = 1..N_c} E_{di}(x_i)
\f]

This energy is a sum of itemwise energies and involves no regularization. Let \f$A=(A_j)_{j=1..N_a}\f$ be the set of normalized features
(see \ref Classification_labels). The itemwise energy measures the coherence of the class \f$x_i\f$ at the \f$i^{th}\f$ item and is defined as:

\f[
  E_{di}(x_i) = \sum_{j = 1..N_a} A_j(x_i)
\f]



\subsection Classification_regularized_local Local Smoothing

- `CGAL::Classifier::run_with_local_smoothing()`: this
method is a tradeoff between quality and efficiency (see Figure
\cgalFigureRef{Classification_image}, bottom-left). The minimized
energy is defined as follows:


\f[
  E(x) = \sum_{i = 1..N_c} E_{si}(x_i)
\f]

The itemwise energy \f$E_{di}(x_i)\f$ is replaced by
\f$E_{si}(x_i)\f$ defined on a small local neighborhood \f$N(i)\f$ of
the \f$i^{th}\f$ item:

\f[
  E_{si}(x_i) = \sum_{j = 1..N_a} \frac{\sum_{k \in N(i)} A_j(x_k)}{\left| N(i) \right|}
\f]

This allows to eliminate local noisy variations of assigned
labels. Increasing the size of the neighborhood
increases the noise reduction at the cost of higher computation times.


\subsection Classification_regularized_graphcut Global Regularization (Graph Cut)

- `CGAL::Classifier::run_with_graphcut()`: this method
offers the best quality but requires longer computation time (see
Figure \cgalFigureRef{Classification_image}, bottom-right). The
total energy that is minimized is the sum of the partial data term
\f$E_{di}(x_i)\f$ and of a pairwise interaction energy defined by the
standard Potts model \cgalCite{cgal:l-mrfmi-09} :

\f[
  E(x) = \sum_{i = 1..N_c} E_{di}(x_i) + \gamma \sum_{i \sim j} \mathbf{1}_{x_i \neq x_j}
\f]

where \f$\gamma>0\f$ is the parameter of the Potts model that
quantifies the strengh of the regularization, \f$i \sim j\f$
represents the pairs of neighboring items and
\f$\mathbf{1}_{\{.\}}\f$ the characteristic function.

A Graph Cut based algorithm (Alpha Expansion) is used to quickly reach
an approximate solution close to the global optimum of this energy.

This method allows to consistently segment the input data set in
piecewise constant parts and to correct large wrongly classified
clusters. Increasing \f$\gamma\f$ produces more regular result with a
constant computation time.


\section Classification_point_set_classifier Point Set Classifier

The classification algorithm is designed to be as flexible as possible: users may classify any type of item they need and define their own features and labels. Nevertheless, \cgal provides a predefined specialization of `CGAL::Classifier` to handle common urban point sets: the class `CGAL::Point_set_classifier`. In addition to all the methods inherited from `CGAL::Classifier`, it provides additional features:

- it takes care of generating all needed analysis structures
- it generates all possible features (among all the \cgal predefined ones) based on which property maps are available (it uses colors if available, etc.)
- multiple scales can be used to increase the quality of the results \cgalCite{cgal:hws-fsso3-16}
- input/ouput methods are provided to save and recover a specific configuration (with all features, labels and relationships between them)
- classification can be saved as a PLY format with colors and labels.

Note that using this class in order to classify point sets is not mandatory, as `CGAL::Classifier` can handle points as well. It is mainly provided to simplify the whole process of defining data structures and features in the specific case of urban point sets. Users can still add their own data structures and features within `CGAL::Point_set_classifier` similarly to what can be done with `CGAL::Classifier`.

The example \ref Classification_example_training shows how to generate features and save the configuration and classification.

\section Classification_trainer Trainer

%Classification is based on relationships between features and labels. Each feature has a specific weight and each pair of feature-label has a specific effect. This means that the number of parameters to set up can quickly explode: if 6 features are used to classify between 4 labels, 30 parameters have to be set up (6 weights + 6x4 feature-label relationships).

Though it is possible to set them up one by one, \cgal also provides a trainer class `CGAL::Classification::Trainer` that requires a small set of ground truth items provided by users. More specifically, users must provide, for each label they want to classify, a set of known inliers among the input data set (for example, selecting one roof, one tree and one section of the ground). The training algorithm works as follows:

- for each feature, a range of weights is tested: the effect each feature have on each label is estimated. For a given weight, if a feature has the same effect on each label, it is non-relevant for classification. The range of weights such that the feature is relevant is estimated

- for each feature, uniformly picked weight values are tested and their effects estimated

- each inlier provided by the user is classified using this set of weights and effects

- for each label, the ratio of correctly classified inliers is computed. The minimum of these ratios is used as a score for this set of weights and effects: a ratio of 0.8 means that for each label, at least 80\% of the provided inliers were correctly classified

- the same mechanism is repeated until all features' ranges have been tested. Weights are only changed one by one, the other ones kept to the previous value that gave the best score.

This usually converges to a satisfying solution (see Figure \cgalFigureRef{Classification_trainer_fig}). The number of trials is user defined, set to 300 by default. Using at least 10 times the number of features is advised (for example, at least 300 iterations if 30 attributes are used). If the solution is not satisfying, more inliers can be selected, for example, in a region that the user identifies as misclassified with the current configuration. The training algorithm keeps the best weights found as initialization and carries on trying new weights by taking new inliers into account.

\cgalFigureBegin{Classification_trainer_fig,classif_training.png}
Example of evolution of training score. The purple curve is the score computed at the current iteration, green curve is the best score found so far.
\cgalFigureEnd

Tools to evaluate the reliability of the training are provided, namely:

- [precision()](@ref CGAL::Classification::Trainer::precision) computes, for one label, the ratio of true positives over the total number of detected positives
- [recall()](@ref CGAL::Classification::Trainer::recall) computes, for one label, the ratio of true positives over the total number of provided inliers of this label
- [f1_score()](@ref CGAL::Classification::Trainer::f1_score) is the harmonic mean of precision and recall
- [IoU()](@ref CGAL::Classification::Trainer::IoU) (Intersection over Union) computes the ratio of true positives over the union of the detected positives and of the provided inliers
- [accuracy()](@ref CGAL::Classification::Trainer::accuracy) computes the ratio of all true positives over the total number of provided inliers
- [mean_f1_score()](@ref CGAL::Classification::Trainer::mean_f1_score)
- [mean_IoU()](@ref CGAL::Classification::Trainer::mean_IoU)


Note that the [Trainer](@ref CGAL::Classification::Trainer) class only uses documented public methods of the [Classifier](@ref CGAL::Classifier), [Label](@ref CGAL::Classification::Label) and [Feature](@ref CGAL::Classification::Feature_base) classes (for example, [set_weight()](@ref CGAL::Classification::Feature_base::set_weight), [set_feature_effect()](@ref CGAL::Classification::Label::set_feature_effect) or [energy_of()](@ref CGAL::Classifier::energy_of)). Users can therefore define their own classes for training using these methods to set up weights and effects and to estimate what are the best parameters.



The example \ref Classification_example_training shows how to set inliers and run the training algorithm.

\section Classification_examples Examples

\subsection Classification_example_general General classification

The following example:

- reads an input file (LIDAR point set in PLY format)
- computes useful structures from this input
- computes segmentation features from the input and the precomputed structures
- defines 3 labels (vegetation, ground and roof) along with the effects of features on them
- classifies the point set using `CGAL::Classifier`
- saves the result in a colored PLY format.

\cgalExample{Classification/example_classifier.cpp}

\subsection Classification_example_training Point set classification with training

The following example:

- reads a point set with a training set (embedded as a PLY feature _label_)
- uses `CGAL::Point_set_classifier` to generate features on 5 scales
- trains the algorithm using 800 trials
- runs the algorithm using the graphcut regularization
- saves the output to PLY.

\cgalExample{Classification/example_point_set_classifier.cpp}


\section Classification_history History

This package is based on a research code by Florent Lafarge that was generalized, extended and packaged by Simon Giraudot.



*/
} /* namespace CGAL */
