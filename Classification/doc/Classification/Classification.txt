namespace CGAL {
/*!

\mainpage User Manual
\anchor Chapter_Classification
\cgalAutoToc
\author Simon Giraudot, Florent Lafarge

This component implements the algorithm described in \cgalCite{cgal:lm-clscm-12} (section 2), generalized to handle multiple features and multiple labels. It classifies a data set into a user-defined set of labels, such as ground, vegetation and buildings. A flexible API is provided so that the user can classify any type of data, compute its own local features on the input data set and define its own labels based on these features.

\section Classification_Organization Package Organization

%Classification of data sets is achieved as follows (see Figure \cgalFigureRef{Classification_organization_fig}):

- some analysis is performed on the input data set
- features are computed based on this analysis
- a set of labels (for example: ground, building, vegetation) is defined by the user
- a predicate is defined and trained: from th set of values taken by the features at an input item, it gives the probability of this item to belong to one label or another
- classification is computed itemwise using the predicate
- additional regularization can be used by smoothing either locally or globally through an Alpha Expansion approach.

\cgalFigureBegin{Classification_organization_fig,organization.png}
Organization of the package.
\cgalFigureEnd

This package is designed to be easily extended by users: more specifically, features and labels can be defined by users to handle any data they need to classify (although \cgal provides predefined features for common urban scenes).

Currently, \cgal provides data structures to handle classification of point sets.

\section Classification_structures Data Structures

\subsection Classification_labels Label set

A label represents how an item should be classified, for example: vegetation, building, road, etc. In \cgal, a label has a name and is simply identified by a [Label_handle](@ref CGAL::Classification::Label_handle).

The following code snippet shows how to add labels to the classification object.

\snippet Classification/example_classification.cpp Labels

\subsection Classification_features Feature set

Features are defined as scalar fields that associate each input item with a specific value. A feature has a name and is identified by a [Feature_handle](@ref CGAL::Classification::Feature_handle).

\cgal provides some predefined features that are relevant for classification of urban point sets:

- [Distance_to_plane](@ref CGAL::Classification::Feature::Distance_to_plane) measures how far away a point is from a locally estimated plane
- [Elevation](@ref CGAL::Classification::Feature::Elevation) computes the local distance to an estimation of the ground
- [Vertical_dispersion](@ref CGAL::Classification::Feature::Vertical_dispersion) computes how noisy the point set is on a local Z-cylinder
- [Verticality](@ref CGAL::Classification::Feature::Verticality) compares the local normal vector to the vertical vector.

For more details about how these different features can help to identify one label or the other, please refer to their associated reference manual pages. In addition, \cgal also provides features solely based on the eigenvalues \cgalCite{cgal:mbrsh-raofw-11} of the local covariance matrix:

- [Anisotropy](@ref CGAL::Classification::Feature::Anisotropy)
- [Eigentropy](@ref CGAL::Classification::Feature::Eigentropy)
- [Linearity](@ref CGAL::Classification::Feature::Linearity)
- [Omnivariance](@ref CGAL::Classification::Feature::Omnivariance)
- [Planarity](@ref CGAL::Classification::Feature::Planarity)
- [Sphericity](@ref CGAL::Classification::Feature::Sphericity)
- [Sum_eigenvalues](@ref CGAL::Classification::Feature::Sum_eigenvalues)
- [Surface_variation](@ref CGAL::Classification::Feature::Surface_variation)

Finally, if the input data set has additional properties, these can also be used as features. For example, \cgal provides the following features:

- [Echo_scatter](@ref CGAL::Classification::Feature::Echo_scatter) uses the number of returns (echo) provided by most LIDAR scanners if available
- [Hsv](@ref CGAL::Classification::Feature::Hsv) uses input color information if available.

In the following code snippet, a subset of these features are instantiated. Note that all the predefined features can also be automatically generated in multiple scales (see \ref Classification_feature_generator).

\snippet Classification/example_classification.cpp Features

Users may want to define their own features, especially if the input data set comes with additional properties that were not anticipated by \cgal. A user-defined feature must inherit from [Feature_base](@ref CGAL::Classification::Feature_base) and provide a method [value()](@ref CGAL::Classification::Feature_base::value) that associate a scalar value to each input item. 

The following example shows how to define a feature that discriminates
points that lie inside a 2D box from the others:

\snippet Classification/example_feature.cpp Feature

This feature can then be instanciated from the feature set the same way as the others:

\snippet Classification/example_feature.cpp Addition

\subsection Classification_analysis Analysis

%Classification is based on the computation of local features. These features can take advantage of shared data structures that are precomputed and stored separately.

\cgal provides the following structures:

- [Point_set_neighborhood](@ref CGAL::Classification::Point_set_neighborhood) stores spatial searching structures and provides adapted queries for points
- [Local_eigen_analysis](@ref CGAL::Classification::Local_eigen_analysis) precomputes covariance matrices on local neighborhoods of points and stores the associated eigenvectors and eigenvalues
- [Planimetric_grid](@ref CGAL::Classification::Planimetric_grid) is a 2D grid used for digital terrain modeling.

The following code snippet shows how to instantiate such data structures from an input PLY point set (the full example is given at the end of the manual).

\snippet Classification/example_classification.cpp Analysis

\subsection Classification_feature_generator Point Set Feature Generator

In standard classification of point sets for urban scenes, users commonly want to use all predefined features to get the best result possible. \cgal provides a class [Point_set_feature_generator](@ref CGAL::Classification::Point_set_feature_generator) that performs the following operations:

- it takes care of generating all needed analysis structures
- it generates all possible features (among all the \cgal predefined ones) based on which property maps are available (it uses colors if available, etc.)
- multiple scales can be used to increase the quality of the results \cgalCite{cgal:hws-fsso3-16}
- if \cgal is linked with TBB, features can be computed in parallel to increase the overall computation speed

Note that using this class in order to generate features is not mandatory, as features and data structures can all be handled by hand. It is mainly provided to make the specific case of urban point sets simpler to handle. Users can still add their own features within their feature set.

The following snippet shows how to use the feature generator:

\snippet Classification/example_generation_and_training.cpp Generator

\section Classification_predicate Predicate

%Classification relies on a predicate: this predicate is an object that, from the set of values taken by the features at an input item, computes the probability that this input item belongs to one label or another. The concept `CGAL::ClassificationPredicate` takes the index of an input item and stores the probability associated to each label in a vector.

For convenience reasons, we hereafter handle energies instead of probabilities, which should be considered as a priority measure _Ã  la STL_: small energy values correspond to large probabilities and large energy values to small probabilities. If a predicate returns the value 0 for a pair of label and input item, it means that this item belongs to this label with certainty.

\cgal provides 2 models for this concept, [Sum_of_weighted_features_predicate](@ref CGAL::Classification::Sum_of_weighted_features_predicate) and [Random_forest_predicate](@ref CGAL::Classification::Random_forest_predicate).

\subsection Classification_sowf Sum of Weighted Features

This first predicate defines the following attributes:

- a weight applied to each feature
- an effect applied to each pair of feature and label

For each label, the predicate computes the energy as a sum of features normalized with both their weight and the effect they have on this specific label.

This predicate can be set up by hand but also embeds a training algorithm.

\subsubsection Classification_sowf_weights_effects Weights and Effects

Each feature is assigned a weight that measure its strength with respect to the other features.

Each pair of feature and label is assigned an effect that can either be:

- [FAVORING](@ref CGAL::Classification::Sum_of_weighted_features_predicate::FAVORING): the label is favored by high values of the feature
- [NEUTRAL](@ref CGAL::Classification::Sum_of_weighted_features_predicate::NEUTRAL): the label is not affected by the feature
- [PENALIZING](@ref CGAL::Classification::Sum_of_weighted_features_predicate::PENALIZING): the label is favored by low values of the feature

For example, vegetation is expected to have a high distance to plane and have a color close to green (if colors are available); facades have a low distance to plane and a low verticality; etc.

Let \f$x=(x_i)_{i=1..N_c}\f$ be a potential classification result with \f$N_c\f$ the number of input items and \f$x_i\f$ the class of the \f$i^{th}\f$ item (for example: vegetation, ground, etc.). Let \f$a_j(i)\f$ be the raw value of the \f$j^{th}\f$ feature at the \f$i^{th}\f$ item and \f$w_j\f$ be the weight of this feature. We define the normalized value \f$A_j(x_i) \in [0:1]\f$ of the \f$j^{th}\f$ feature at the \f$i^{th}\f$ item as follows:

\f{eqnarray*}{
    A_j(x_i) = & (1 - \min(\max(0,\frac{a_j(i)}{w_j}), 1)) & \mbox{if } a_j \mbox{ favors } x_i \\
    & 0.5 & \mbox{if } a_j \mbox{ is neutral for } x_i \\
    & \min(\max(0,\frac{a_j(i)}{w_j}), 1) & \mbox{if } a_j \mbox{ penalizes } x_i
    \f}

The following code snippet shows how to define the weights and effects of features and labels:

\snippet Classification/example_classification.cpp Weights

\subsubsection Classification_sowf_training Training

Each feature has a specific weight and each pair of feature-label has a specific effect. This means that the number of parameters to set up can quickly explode: if 6 features are used to classify between 4 labels, 30 parameters have to be set up (6 weights + 6x4 feature-label relationships).

Though it is possible to set them up one by one, \cgal also provides a method [train()](@ref CGAL::Classification::Sum_of_weighted_features_predicate::train)  that requires a small set of ground truth items provided by users. More specifically, users must provide, for each label they want to classify, a set of known inliers among the input data set (for example, selecting one roof, one tree and one section of the ground). The training algorithm works as follows:

- for each feature, a range of weights is tested: the effect each feature have on each label is estimated. For a given weight, if a feature has the same effect on each label, it is non-relevant for classification. The range of weights such that the feature is relevant is estimated

- for each feature, uniformly picked weight values are tested and their effects estimated

- each inlier provided by the user is classified using this set of weights and effects

- the mean intersection over union (see @ref Classification_evaluation) is used to evaluate the quality of this set of weights and effects

- the same mechanism is repeated until all features' ranges have been tested. Weights are only changed one by one, the other ones kept to the previous value that gave the best score.

This usually converges to a satisfying solution (see Figure \cgalFigureRef{Classification_trainer_fig}). The number of trials is user defined, set to 300 by default. Using at least 10 times the number of features is advised (for example, at least 300 iterations if 30 attributes are used). If the solution is not satisfying, more inliers can be selected, for example, in a region that the user identifies as misclassified with the current configuration. The training algorithm keeps the best weights found as initialization and carries on trying new weights by taking new inliers into account.

\cgalFigureBegin{Classification_trainer_fig,classif_training.png}
Example of evolution of training score. The purple curve is the score computed at the current iteration, green curve is the best score found so far.
\cgalFigureEnd

\subsection Classification_random_forest Random Forest

This second predicate is [Random_forest_predicate](@ref CGAL::Classification::Random_forest_predicate).
It uses the \ref thirdpartyOpenCV library, more specifically the
[Random Trees](http://docs.opencv.org/2.4/modules/ml/doc/random_trees.html)
package.

This predicate uses a ground truth training set to construct several
decision trees that are then used to assign a label to each input
item.

This predicate cannot be set up by hand and requires a ground truth
training set. The training algorithm usually requires more inliers
than the one of the previous predicate but is faster.

Note that so far, \ref thirdpartyOpenCV does not provide input/output
functions for random trees, which means the result of the training
cannot be saved and recovered for further classification.

For more details about this method, please refer to [the official
  documentation](http://docs.opencv.org/2.4/modules/ml/doc/random_trees.html)
of OpenCV.


\section Classification_classifiers Classifiers

%Classification is performed by minizing an energy over the input data set that may include regularization. \cgal provides 3 different methods for classification, ranging from high speed / low quality to low speed / high quality:

- `CGAL::Classification::classify()` 
- `CGAL::Classification::classify_with_local_smoothing()`
- `CGAL::Classification::classify_with_graphcut()`

On a point set of 3 millions of points, the first method takes about 4 seconds, the second about 40 seconds and the third about 2 minutes.

\cgalFigureBegin{Classification_image,classif.png}
Top-Left: input point set. Top-Right: raw output classification represented by a set of colors (ground is orange, facades are blue, roofs are pink and vegetation is green). Bottom-Left: output classification using local smoothing. Bottom-Right: output classification using graphcut.
\cgalFigureEnd


Mathematical details are provided hereafter.

\subsection Classification_classify Raw classification

- `CGAL::Classification::classify()`: this is the fastest method
that provides acceptable but usually noisy results (see Figure
\cgalFigureRef{Classification_image}, top-right).

Let \f$x=(x_i)_{i=1..N_c}\f$ be a potential classification result with \f$N_c\f$ the number of input items and \f$x_i\f$ the label of the \f$i^{th}\f$ item (for example: vegetation, ground, etc.). The classification is performed by minimizing the following energy:

\f[
  E(x) = \sum_{i = 1..N_c} E_{di}(x_i)
\f]

This energy is a sum of itemwise energies and involves no regularization. Let \f$A=(A_j)_{j=1..N_a}\f$ be the set of normalized features
(see \ref Classification_labels). The itemwise energy measures the coherence of the label \f$x_i\f$ at the \f$i^{th}\f$ item and is defined as:

\f[
  E_{di}(x_i) = \sum_{j = 1..N_a} A_j(x_i)
  \f]

The following snippets show how to classify points based on a label
set and a predicate. The result is stored in `label_indices`,
following the same order as the input set and providing for each point
the index (in the label set) of its assigned label.

\snippet Classification/example_classification.cpp Classify

\subsection Classification_smoothing Local Regularization

- `CGAL::Classification::classify_with_local_smoothing()`: this
method is a tradeoff between quality and efficiency (see Figure
\cgalFigureRef{Classification_image}, bottom-left). The minimized
energy is defined as follows:


\f[
  E(x) = \sum_{i = 1..N_c} E_{si}(x_i)
\f]

The itemwise energy \f$E_{di}(x_i)\f$ is replaced by
\f$E_{si}(x_i)\f$ defined on a small local neighborhood \f$N(i)\f$ of
the \f$i^{th}\f$ item:

\f[
  E_{si}(x_i) = \sum_{j = 1..N_a} \frac{\sum_{k \in N(i)} A_j(x_k)}{\left| N(i) \right|}
\f]

This allows to eliminate local noisy variations of assigned
labels. Increasing the size of the neighborhood
increases the noise reduction at the cost of higher computation times.

The following snippets show how to classify points using local
smoothing by providing a model of `CGAL::NeighborQuery`.

\snippet Classification/example_classification.cpp Smoothing

\subsection Classification_graphcut Global Regularization (Graph Cut)

- `CGAL::Classification::classify_with_graphcut()`: this method
offers the best quality but requires longer computation time (see
Figure \cgalFigureRef{Classification_image}, bottom-right). The
total energy that is minimized is the sum of the partial data term
\f$E_{di}(x_i)\f$ and of a pairwise interaction energy defined by the
standard Potts model \cgalCite{cgal:l-mrfmi-09} :

\f[
  E(x) = \sum_{i = 1..N_c} E_{di}(x_i) + \gamma \sum_{i \sim j} \mathbf{1}_{x_i \neq x_j}
\f]

where \f$\gamma>0\f$ is the parameter of the Potts model that
quantifies the strengh of the regularization, \f$i \sim j\f$
represents the pairs of neighboring items and
\f$\mathbf{1}_{\{.\}}\f$ the characteristic function.

A Graph Cut based algorithm (Alpha Expansion) is used to quickly reach
an approximate solution close to the global optimum of this energy.

This method allows to consistently segment the input data set in
piecewise constant parts and to correct large wrongly classified
clusters. Increasing \f$\gamma\f$ produces more regular result with a
constant computation time.

To speed up computation, the input domain can be subdivided into
smaller subsets such that several smaller graph cuts are applied
instead of a big one. The computation of these smaller graph cuts can
be done in parallel. Increasing the number of subsets allows for
faster computation times but can also reduce the quality of the
results.

The following snippets show how to classify points using a graph cut
regularization providing a model of `CGAL::NeighborQuery`, a strengh
parameter \f$\gamma\f$ and a number of subdivisions.

\snippet Classification/example_classification.cpp Graph_cut

\section Classification_evaluation Evaluation

The class [Evaluation](@ref CGAL::Classification::Evaluation) allows uers to evaluate the reliability of the classification with respect to a provided ground truth. The following measurements are available:

- [precision()](@ref CGAL::Classification::Evaluation::precision) computes, for one label, the ratio of true positives over the total number of detected positives
- [recall()](@ref CGAL::Classification::Evaluation::recall) computes, for one label, the ratio of true positives over the total number of provided inliers of this label
- [f1_score()](@ref CGAL::Classification::Evaluation::f1_score) is the harmonic mean of precision and recall
- [intersection_over_union()](@ref CGAL::Classification::Evaluation::intersection_over_union) computes the ratio of true positives over the union of the detected positives and of the provided inliers
- [accuracy()](@ref CGAL::Classification::Evaluation::accuracy) computes the ratio of all true positives over the total number of provided inliers
- [mean_f1_score()](@ref CGAL::Classification::Evaluation::mean_f1_score)
- [mean_intersection_over_union()](@ref CGAL::Classification::Evaluation::mean_intersection_over_union)

All of these values range from 0 (poor quality) to 1 (perfect quality).

\section Classification_examples Full Examples

\subsection Classification_example_general Classification

The following example:

- reads an input file (LIDAR point set in PLY format)
- computes useful structures from this input
- computes features from the input and the precomputed structures
- defines 3 labels (vegetation, ground and roof)
- sets up the classification predicate [Sum_of_weighted_features_predicate](@ref CGAL::Classification::Sum_of_weighted_features_predicate)
- classifies the point set with the 3 different methods (this is for the sake of the example: each method overwrite the previous result, users should onluy call one of the methods)
- saves the result in a colored PLY format.

\cgalExample{Classification/example_classification.cpp}

\subsection Classification_example_feeature Defining a Custom Feature

The following example shows how to define a custom feature and how to integrate it in the \cgal classification framework.

\cgalExample{Classification/example_feature.cpp}

\subsection Classification_example_training Feature Generation and Training

The following example:

- reads a point set with a training set (embedded as a PLY feature _label_)
- automatically generates features on 5 scales
- trains the predicate [Sum_of_weighted_features_predicate](@ref CGAL::Classification::Sum_of_weighted_features_predicate) using 800 trials
- runs the algorithm using the graphcut regularization
- prints some evaluation measurements
- saves the configuration of the predicate for further use.

\cgalExample{Classification/example_generation_and_training.cpp}

\subsection Classification_example_random_forest Random Forest

The following example shows how to use the predicate [Random_forest_predicate](@ref CGAL::Classification::Random_forest_predicate) using an input training set.

\cgalExample{Classification/example_random_forest.cpp}

\section Classification_history History

This package is based on a research code by Florent Lafarge that was generalized, extended and packaged by Simon Giraudot.



*/
} /* namespace CGAL */
